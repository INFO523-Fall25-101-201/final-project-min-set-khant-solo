[
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "INFO-523-DATA MININIG_Final Project ‘BMW Car Price Analysis & Prediction (Project Proposal)’",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "proposal.html#dataset",
    "href": "proposal.html#dataset",
    "title": "INFO-523-DATA MININIG_Final Project ‘BMW Car Price Analysis & Prediction (Project Proposal)’",
    "section": "Dataset",
    "text": "Dataset\n\ndata = pd.read_csv(\"data/bmw_worldwide_sales.csv\")\ndata.head(10)\n\n\n\n\n\n\n\n\nModel\nYear\nRegion\nColor\nFuel_Type\nTransmission\nEngine_Size_L\nMileage_KM\nPrice_USD\nSales_Volume\nSales_Classification\n\n\n\n\n0\n5 Series\n2016\nAsia\nRed\nPetrol\nManual\n3.5\n151748\n98740\n8300\nHigh\n\n\n1\ni8\n2013\nNorth America\nRed\nHybrid\nAutomatic\n1.6\n121671\n79219\n3428\nLow\n\n\n2\n5 Series\n2022\nNorth America\nBlue\nPetrol\nAutomatic\n4.5\n10991\n113265\n6994\nLow\n\n\n3\nX3\n2024\nMiddle East\nBlue\nPetrol\nAutomatic\n1.7\n27255\n60971\n4047\nLow\n\n\n4\n7 Series\n2020\nSouth America\nBlack\nDiesel\nManual\n2.1\n122131\n49898\n3080\nLow\n\n\n5\n5 Series\n2017\nMiddle East\nSilver\nDiesel\nManual\n1.9\n171362\n42926\n1232\nLow\n\n\n6\ni8\n2022\nEurope\nWhite\nDiesel\nManual\n1.8\n196741\n55064\n7949\nHigh\n\n\n7\nM5\n2014\nAsia\nBlack\nDiesel\nAutomatic\n1.6\n121156\n102778\n632\nLow\n\n\n8\nX3\n2016\nSouth America\nWhite\nDiesel\nAutomatic\n1.7\n48073\n116482\n8944\nHigh\n\n\n9\ni8\n2019\nEurope\nWhite\nElectric\nManual\n3.0\n35700\n96257\n4411\nLow"
  },
  {
    "objectID": "proposal.html#dataset-overview",
    "href": "proposal.html#dataset-overview",
    "title": "INFO-523-DATA MININIG_Final Project ‘BMW Car Price Analysis & Prediction (Project Proposal)’",
    "section": "Dataset Overview",
    "text": "Dataset Overview\n\ndata.describe()\ndata.shape\n\n(50000, 11)\n\n\nThis dataset — BMW Worldwide Sales Records (2010–2024) — contains over 50,000 records of BMW’s sales and specifications across multiple regions. Key features include: Model, Year, Engine_Size_L, Transmission, Fuel_Type, Color, Region, Price, and Sales_Volume. This dataset was chosen because it provides a diverse range of attributes for exploring market behavior, pricing trends, and customer preferences in the automotive industry."
  },
  {
    "objectID": "proposal.html#questions",
    "href": "proposal.html#questions",
    "title": "INFO-523-DATA MININIG_Final Project ‘BMW Car Price Analysis & Prediction (Project Proposal)’",
    "section": "Questions",
    "text": "Questions\n\nWhat are the key factors influencing BMW used-car prices in the market?\nCan machine learning models accurately predict used-car prices?\nHow stable are these predictions over time?\nUsing a temporal split, how well can the model predict pricing trends for “next year”?"
  },
  {
    "objectID": "proposal.html#analysis-plan",
    "href": "proposal.html#analysis-plan",
    "title": "INFO-523-DATA MININIG_Final Project ‘BMW Car Price Analysis & Prediction (Project Proposal)’",
    "section": "Analysis Plan",
    "text": "Analysis Plan\n\nData Cleaning & Preparation\n\nHandle missing values\nStandardize numerical units\nEncode categorical variables\nEnsure time-related variables are aligned for temporal analysis\n\nExploratory Data Analysis (EDA)\n\nPrice trends by year, region, and model\nCorrelation analysis between price and vehicle attributes\nIdentify patterns that may indicate shifting market preferences\n\nModeling Approach\n\nApply Machine Learning Models:\nLinear Regression\nDecision Tree Regression\nRandom Forest Regression\nEvaluate using:\nRMSE, MAE, R²\nPerform:\nFeature importance analysis\nTemporal train–test split to simulate predicting next-year price trends\n\nVisualization Dashboard\n\nBuild clear, interactive visualizations using matplotlib / seaborn\nInclude:\nPrice distribution\nTrend lines across years\nModel performance summary\nVisualization Dashboard:\n\nBuild clear, interactive plots for trends and insights using matplotlib and seaborn.\n\nEthical AI Use Disclosure:\n\nAI tools (e.g., ChatGPT) were used ethically for code debugging, idea exploration, and documentation enhancement.\nAll datasets are real and sourced from Kaggle."
  },
  {
    "objectID": "all_notebook/01_Data_Preparation_and_EDA-.html",
    "href": "all_notebook/01_Data_Preparation_and_EDA-.html",
    "title": "BMW Car Sales — Data Preparation & EDA",
    "section": "",
    "text": "#### Setup, Imports, and Data Loading\n\n# 1. SETUP AND IMPORTS\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom pathlib import Path\nfrom scipy import stats\n\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\"display.max_columns\", None)\nsns.set(style=\"whitegrid\")\n%matplotlib inline\n\n# Load the Data and Set Paths\nDATA_PATH = Path(\"..\") / \"data\" / \"bmw_worldwide_sales.csv\"\nOUTPUT_DIR = Path(\"..\") / \"data\" / \"cleaned\"\nVISUALS_DIR = Path(\"..\") / \"visuals\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(VISUALS_DIR, exist_ok=True)\n\n# Load Data\ntry:\n    df = pd.read_csv(DATA_PATH)\nexcept FileNotFoundError:\n    print(f\"{DATA_PATH} not found. Ensure the file is in the correct path.\")\n    df = pd.DataFrame() # Create empty DF to prevent errors\n\nif not df.empty:\n    print(\"Raw shape:\", df.shape)\n    display(df.head(10))\n    df.info()\n\nRaw shape: (50000, 11)\n\n\n\n\n\n\n\n\n\nModel\nYear\nRegion\nColor\nFuel_Type\nTransmission\nEngine_Size_L\nMileage_KM\nPrice_USD\nSales_Volume\nSales_Classification\n\n\n\n\n0\n5 Series\n2016\nAsia\nRed\nPetrol\nManual\n3.5\n151748\n98740\n8300\nHigh\n\n\n1\ni8\n2013\nNorth America\nRed\nHybrid\nAutomatic\n1.6\n121671\n79219\n3428\nLow\n\n\n2\n5 Series\n2022\nNorth America\nBlue\nPetrol\nAutomatic\n4.5\n10991\n113265\n6994\nLow\n\n\n3\nX3\n2024\nMiddle East\nBlue\nPetrol\nAutomatic\n1.7\n27255\n60971\n4047\nLow\n\n\n4\n7 Series\n2020\nSouth America\nBlack\nDiesel\nManual\n2.1\n122131\n49898\n3080\nLow\n\n\n5\n5 Series\n2017\nMiddle East\nSilver\nDiesel\nManual\n1.9\n171362\n42926\n1232\nLow\n\n\n6\ni8\n2022\nEurope\nWhite\nDiesel\nManual\n1.8\n196741\n55064\n7949\nHigh\n\n\n7\nM5\n2014\nAsia\nBlack\nDiesel\nAutomatic\n1.6\n121156\n102778\n632\nLow\n\n\n8\nX3\n2016\nSouth America\nWhite\nDiesel\nAutomatic\n1.7\n48073\n116482\n8944\nHigh\n\n\n9\ni8\n2019\nEurope\nWhite\nElectric\nManual\n3.0\n35700\n96257\n4411\nLow\n\n\n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 11 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   Model                 50000 non-null  object \n 1   Year                  50000 non-null  int64  \n 2   Region                50000 non-null  object \n 3   Color                 50000 non-null  object \n 4   Fuel_Type             50000 non-null  object \n 5   Transmission          50000 non-null  object \n 6   Engine_Size_L         50000 non-null  float64\n 7   Mileage_KM            50000 non-null  int64  \n 8   Price_USD             50000 non-null  int64  \n 9   Sales_Volume          50000 non-null  int64  \n 10  Sales_Classification  50000 non-null  object \ndtypes: float64(1), int64(4), object(6)\nmemory usage: 4.2+ MB\n\n\n\n\n\n\n# Fix numeric types & simple conversions\nnum_cols = [\"Year\",\"Engine_Size_L\",\"Mileage_KM\",\"Price_USD\",\"Sales_Volume\"]\nfor c in num_cols:\n    if c in df.columns:\n        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n\n# Quick check\nprint(df[num_cols].isna().sum())\n\nYear             0\nEngine_Size_L    0\nMileage_KM       0\nPrice_USD        0\nSales_Volume     0\ndtype: int64\n\n\n\n# Derived features\nCURRENT_YEAR = 2025\ndf[\"Car_Age\"] = CURRENT_YEAR - df[\"Year\"]\ndf[\"log_Price_USD\"] = np.log1p(df[\"Price_USD\"]) # Standardized name for target\ndf[\"Price_per_KM\"] = df[\"Price_USD\"] / df[\"Mileage_KM\"].replace(0, np.nan)\ndf[\"Price_per_KM\"] = df[\"Price_per_KM\"].fillna(df[\"Price_per_KM\"].median())\n\n\n# Categorical Normalization\n\n# Transmission normalization\ndf[\"Transmission\"] = df[\"Transmission\"].replace({\n    \"Auto\": \"Automatic\",\n    \"Man\": \"Manual\",\n    \"Automated Manual\": \"Automatic\",\n}).fillna(\"Unknown\")\n\n# Fuel check (ensure consistency, fill unknown)\ndf[\"Fuel_Type\"] = df[\"Fuel_Type\"].fillna(\"Other\")\n\n\n# Missing values & duplicates handling\nprint(\"Duplicates before:\", df.duplicated().sum())\ndf = df.drop_duplicates()\nprint(\"Duplicates after:\", df.duplicated().sum())\n\n# Impute Engine_Size_L per Model, fallback to global median\nif \"Engine_Size_L\" in df.columns and \"Model\" in df.columns:\n    df[\"Engine_Size_L\"] = df.groupby(\"Model\")[\"Engine_Size_L\"].transform(lambda x: x.fillna(x.median()))\ndf[\"Engine_Size_L\"] = df[\"Engine_Size_L\"].fillna(df[\"Engine_Size_L\"].median())\n\n# Drop rows missing Price_USD (target)\nbefore = df.shape[0]\ndf = df[~df[\"Price_USD\"].isna()].copy()\nafter = df.shape[0]\nprint(f\"Dropped {before-after} rows with missing Price_USD.\")\n\nDuplicates before: 0\nDuplicates after: 0\nDropped 0 rows with missing Price_USD.\n\n\n\n# Outlier check (as per proposal, no transformation needed for now)\ndef iqr_bounds(s, k=1.5):\n    q1, q3 = np.nanpercentile(s, [25,75])\n    iqr = q3 - q1\n    return q1 - k*iqr, q3 + k*iqr\n\nfor c in [\"Price_USD\",\"Mileage_KM\",\"Engine_Size_L\",\"Sales_Volume\"]:\n    if c in df.columns:\n        low, high = iqr_bounds(df[c].dropna(), k=1.5)\n        pct_out = ((df[c] &lt; low) | (df[c] &gt; high)).mean() * 100\n        print(f\"{c}: IQR bounds [{low:.2f}, {high:.2f}], outliers ~ {pct_out:.3f}% \")\n\nPrice_USD: IQR bounds [-15355.50, 165418.50], outliers ~ 0.000% \nMileage_KM: IQR bounds [-100500.38, 301308.62], outliers ~ 0.000% \nEngine_Size_L: IQR bounds [-0.15, 6.65], outliers ~ 0.000% \nSales_Volume: IQR bounds [-4835.88, 14961.12], outliers ~ 0.000% \n\n\n\n\n\n\n# Additional Feature Engineering (for EDA and modeling)\n\n# Engine size bins\ndf[\"Engine_Bin\"] = pd.cut(df[\"Engine_Size_L\"], bins=[0,1.6,2.0,3.0,4.0,10],\n                            labels=[\"&lt;=1.6\",\"1.7-2.0\",\"2.1-3.0\",\"3.1-4.0\",\"&gt;4.0\"])\n# Age bins\ndf[\"Age_Bin\"] = pd.cut(df[\"Car_Age\"], bins=[-1,1,3,6,10,100], labels=[\"0-1\",\"2-3\",\"4-6\",\"7-10\",\"10+\"])\n# Model popularity (Total Sales)\nmodel_sales = df.groupby(\"Model\")[\"Sales_Volume\"].sum().rename(\"Total_Sales_Model\").reset_index()\ndf = df.merge(model_sales, on=\"Model\", how=\"left\")\n\n\n# Save cleaned CSV (for dashboard/visuals)\nclean_path = OUTPUT_DIR / \"bmw_cleaned.csv\"\ndf.to_csv(clean_path, index=False)\nprint(\"Saved cleaned CSV to:\", clean_path)\n\nSaved cleaned CSV to: ../data/cleaned/bmw_cleaned.csv"
  },
  {
    "objectID": "all_notebook/01_Data_Preparation_and_EDA-.html#exploratory-data-analysis-eda-visualizations",
    "href": "all_notebook/01_Data_Preparation_and_EDA-.html#exploratory-data-analysis-eda-visualizations",
    "title": "BMW Car Sales — Data Preparation & EDA",
    "section": "Exploratory Data Analysis (EDA) Visualizations",
    "text": "Exploratory Data Analysis (EDA) Visualizations\n\n# Setup/Imports \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\n\nsns.set(style=\"whitegrid\")\nVISUALS_DIR = Path(\"..\") / \"visuals\" \nos.makedirs(VISUALS_DIR, exist_ok=True) \n\n# Load the CLEANED data for visualizations\nDATA_PATH = Path(\"..\") / \"data\" / \"cleaned\" / \"bmw_cleaned.csv\"\ntry:\n    df = pd.read_csv(DATA_PATH)\nexcept FileNotFoundError:\n    print(\"Error: Cleaned data not found.\")\n\n\n# Price Distribution (USD)\nplt.figure(figsize=(10, 6))\nsns.histplot(df[\"Price_USD\"], kde=True, bins=50)\nplt.title(\"Distribution of BMW Car Prices (USD)\", fontsize=16)\nplt.xlabel(\"Price (USD)\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.savefig(VISUALS_DIR / \"price_distribution.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Log-Price Distribution\nplt.figure(figsize=(10, 6))\nsns.histplot(df[\"log_Price_USD\"], kde=True, bins=50, color='darkorange')\nplt.title(\"Distribution of Log-Transformed Price (Target)\", fontsize=16)\nplt.xlabel(\"Log(1 + Price_USD)\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.savefig(VISUALS_DIR / \"log_price_distribution.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Average Price Trend by Year\nplt.figure(figsize=(12, 6))\navg_price_year = df.groupby(\"Year\")[\"Price_USD\"].mean().reset_index()\nsns.lineplot(x=\"Year\", y=\"Price_USD\", data=avg_price_year, marker='o', color='navy')\nplt.title(\"Average BMW Car Price Trend Over Time\", fontsize=16)\nplt.xlabel(\"Year\")\nplt.ylabel(\"Average Price (USD)\")\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig(VISUALS_DIR / \"avg_price_by_year.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Price by Fuel Type\nplt.figure(figsize=(12, 7))\nsns.boxplot(x=\"Fuel_Type\", y=\"Price_USD\", data=df, palette=\"Pastel1\", showfliers=False)\nplt.title(\"Price Distribution by Fuel Type\", fontsize=16)\nplt.xlabel(\"Fuel Type\")\nplt.ylabel(\"Price (USD)\")\nplt.tight_layout()\nplt.savefig(VISUALS_DIR / \"price_by_fueltype_box.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Top 10 Models Globally (Sales Volume)\nplt.figure(figsize=(14, 7))\n# Calculate total sales volume per model\ntop_models = df.groupby('Model')['Sales_Volume'].sum().nlargest(10).sort_values(ascending=False).reset_index()\n\nsns.barplot(x='Sales_Volume', y='Model', data=top_models, palette='rocket')\nplt.title(\"Top 10 BMW Models by Global Sales Volume\", fontsize=16)\nplt.xlabel(\"Total Sales Volume\")\nplt.ylabel(\"Model\")\nplt.tight_layout()\nplt.savefig(VISUALS_DIR / \"top10_models_global.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Correlation Matrix (Heatmap)\n# Selecting key numerical/quasi-numerical features\ncorr_cols = ['log_Price_USD', 'Price_USD', 'Mileage_KM', 'Engine_Size_L', 'Car_Age', 'Sales_Volume', 'Price_per_KM']\ncorr_df = df[corr_cols].dropna()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_df.corr(), annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=.5, linecolor='black')\nplt.title(\"Correlation Matrix of Key Numerical Variables\", fontsize=16)\nplt.tight_layout()\nplt.savefig(VISUALS_DIR / \"correlation_matrix.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Top Models by Region (Sales Volume)\n# Calculate the top 5 models per region based on total sales volume\ndef get_top_models(group):\n    return group.nlargest(5, 'Total_Sales_Model')\n\ntop_models_region = df.groupby(['Region', 'Model'])['Sales_Volume'].sum().reset_index()\ntop_models_region = top_models_region.rename(columns={'Sales_Volume': 'Total_Sales_Model'})\ntop_models_per_region = top_models_region.groupby('Region', group_keys=False).apply(get_top_models).reset_index(drop=True)\n\nplt.figure(figsize=(14, 8))\nsns.barplot(x='Total_Sales_Model', y='Model', hue='Region', data=top_models_per_region, dodge=False, palette='tab10')\nplt.title(\"Top BMW Models by Sales Volume Across Regions\", fontsize=16)\nplt.xlabel(\"Total Sales Volume\")\nplt.ylabel(\"Model\")\nplt.legend(title='Region', loc='lower right')\nplt.tight_layout()\nplt.savefig(VISUALS_DIR / \"top_model_per_region.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\n4. Final Preparation for Modeling: Temporal Split Readiness\nThe final step prepares the dataset by converting categorical features into numerical ones using One-Hot Encoding (OHE) and dropping redundant columns. This bmw_modeling_ready.csv file is the essential input for the 02-modeling.ipynb notebook.\n\n# Prepare modeling-ready dataset\nmodel_df = df.copy()\n\n# Select columns for One-Hot Encoding (OHE)\ncat_cols = [\"Region\",\"Model\",\"Fuel_Type\",\"Transmission\",\"Color\"]\nmodel_df = pd.get_dummies(model_df, columns=[c for c in cat_cols if c in model_df.columns], drop_first=True)\n\n# Drop original categorical columns\nmodel_df = model_df.drop(columns=[c for c in cat_cols if c in model_df.columns], errors='ignore')\n\n# Drop columns that are redundant/not used in modeling, but were useful for EDA/initial features\n# Note: Price_USD is kept here so it can be extracted in 02-modeling for the final MAE calculation\ncols_to_drop = [\"Sales_Classification\", \"Engine_Bin\", \"Age_Bin\", \"index\"] \nmodel_df = model_df.drop(columns=[c for c in cols_to_drop if c in model_df.columns], errors='ignore')\n\n# Save modeling-ready CSV \nprocessed_path = OUTPUT_DIR / \"bmw_modeling_ready.csv\" \nmodel_df.to_csv(processed_path, index=False)\nprint(\"Saved modeling-ready CSV:\", processed_path)\nprint(\"Final Modeling Shape:\", model_df.shape)\n\nSaved modeling-ready CSV: ../data/cleaned/bmw_modeling_ready.csv\nFinal Modeling Shape: (50000, 33)"
  },
  {
    "objectID": "bmw_predictor.html",
    "href": "bmw_predictor.html",
    "title": "BMW Price Predictor",
    "section": "",
    "text": "BMW Price Predictor Dashboard\n\n    \n    \n\n    \n\n\n\n\n    \n        \n        \n        \n            \n                BMW Price Prediction Interface\n            \n            \n                Simulated model using key features (Mileage, Age, Engine Size) and Price per KM logic.\n            \n\n            \n                \n                \n                \n                    Mileage (KM)\n                    \n                \n\n                \n                \n                    Car Age (Years)\n                    \n                \n\n                \n                \n                    Engine Size (L)\n                    \n                \n\n                \n                \n                    Region\n                    \n                        North America\n                        Europe\n                        Asia\n                        Middle East\n                        South America\n                    \n                \n\n                \n                \n                    Model Series\n                    \n                        7 Series (High End)\n                        5 Series (Mid/Executive)\n                        X5 (Premium SUV)\n                        X1 (Entry SUV)\n                        3 Series (Entry Sedan)\n                    \n                \n\n                \n                \n                    \n                        Predict Market Price\n                    \n                \n            \n        \n\n        \n        \n            \n                Predicted Price\n                \n                    Enter car details and click 'Predict'.\n                \n            \n            \n            \n                \n                Running Random Forest Model...\n            \n            \n            \n\n            \n                MAE (Temporal Validation)\n                $531.13\n                Confirmed accuracy on 2023–2024 unseen data."
  },
  {
    "objectID": "writeup.html",
    "href": "writeup.html",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "",
    "text": "This project focuses on building a robust, temporally validated machine learning model to predict the market price of used BMW vehicles across multiple global regions. The analysis used the BMW Worldwide Sales dataset (2010–2024) and applied Random Forest Regression.\nA key methodological innovation was the use of a Temporal Train–Test Split, where the model was trained only on data up to 2023 and tested exclusively on future sales from 2024.\nThe final model achieved:\n\nR²: 0.952\nMAE: $531.13\nMSE (log scale): 0.0079\n\nThe strongest finding is the dominance of the engineered feature Price per KM, which emerged as the most powerful predictor of BMW car prices, outperforming all original features."
  },
  {
    "objectID": "writeup.html#dataset-overview",
    "href": "writeup.html#dataset-overview",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "2.1 Dataset Overview",
    "text": "2.1 Dataset Overview\nThe dataset consists of BMW Worldwide Sales Records (2010–2024) sourced from Kaggle, containing over 50,000 vehicle records with fields such as:\n\nModel\nMileage\nProduction Year\nRegion\nFuel Type\nTransmission\nPrice in USD\n\n\nCleaning & Transformation Steps\n\nMissing values imputed (categorical → \"Unknown\").\nCar Age computed from sale year.\nLog transformation applied to Price_USD.\nOne-Hot Encoding applied to:\n\nRegion\nModel\nFuel_Type\nTransmission"
  },
  {
    "objectID": "writeup.html#engineered-feature-price-per-km",
    "href": "writeup.html#engineered-feature-price-per-km",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "2.2 Engineered Feature: Price per KM",
    "text": "2.2 Engineered Feature: Price per KM\nThe most critical feature created:\n\\[\n\\text{Price per KM} = \\frac{\\text{Price_USD}}{\\text{Mileage_KM}}\n\\]\nThis variable quantifies value retention efficiency, separating high-quality vehicles from low-value ones despite similar mileage.\nIt ultimately became the single strongest predictor in the model."
  },
  {
    "objectID": "writeup.html#market-price-trend",
    "href": "writeup.html#market-price-trend",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "3.1 Market Price Trend",
    "text": "3.1 Market Price Trend\nPost-2020, BMW used car prices exhibit a sharp upward spike, reflecting:\n\nSupply chain shortages\nInflation\nGlobal demand shifts\n\nThis volatility made temporal validation essential."
  },
  {
    "objectID": "writeup.html#regional-segmentation",
    "href": "writeup.html#regional-segmentation",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "3.2 Regional Segmentation",
    "text": "3.2 Regional Segmentation\nTop models vary significantly by region:\n\nNorth America: SUVs & performance models\nEurope: compact & diesel variants\nAsia: fuel-efficient premium sedans\n\nThis confirmed that Region is a crucial categorical feature."
  },
  {
    "objectID": "writeup.html#baseline-and-final-model",
    "href": "writeup.html#baseline-and-final-model",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "4.1 Baseline and Final Model",
    "text": "4.1 Baseline and Final Model\n\nBaseline: Linear Regression\nFinal Model: Random Forest Regressor\n\nReasons for choosing Random Forest:\n\nHandles nonlinear relationships\nRobust to one-hot encoded features\nProvides interpretable Feature Importance scores\nPerforms well under volatile price conditions"
  },
  {
    "objectID": "writeup.html#temporal-traintest-split",
    "href": "writeup.html#temporal-traintest-split",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "4.2 Temporal Train–Test Split",
    "text": "4.2 Temporal Train–Test Split\nTo simulate real-world forecasting:\n\nTrain: 2010–2023\nTest: 2024\n\nNo future data was allowed during training.\nThis ensured the model learns historical trends and predicts market conditions it has never seen."
  },
  {
    "objectID": "writeup.html#performance-on-future-data-20232024",
    "href": "writeup.html#performance-on-future-data-20232024",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "5.1 Performance on Future Data (2023–2024)",
    "text": "5.1 Performance on Future Data (2023–2024)\n\n\n\n\n\n\n\n\nMetric\nResult\nInterpretation\n\n\n\n\nR²\n0.9952\nExplains ~99% of price variance in future sales\n\n\nMSE\n0.0079\nLow error after log transformation\n\n\nMAE\n$531.13\nAverage price prediction error is only $531\n\n\n\nThis MAE is exceptionally low given BMW’s price range, demonstrating strong temporal stability."
  },
  {
    "objectID": "writeup.html#feature-importance",
    "href": "writeup.html#feature-importance",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "5.2 Feature Importance",
    "text": "5.2 Feature Importance\nRanked by reduction in Gini Impurity:\n\n\n\n\n\n\n\n\n\nRank\nFeature\nImportance\nMeaning\n\n\n\n\n1\nPrice per KM\n0.682\nDominant value-retention indicator\n\n\n2\nMileage (KM)\n0.318\nCore depreciation measure\n\n\n3\nEngine Size (L)\n0.000018\nMinimal impact\n\n\n4\nCar Age\n0.000015\nSmall but logical\n\n\n5\nTransmission: Manual\n0.000004\nNiche effect\n\n\n\nKey insight: The engineered feature Price per KM is more informative than raw Mileage, Age, or Engine Size."
  },
  {
    "objectID": "writeup.html#summary-of-findings",
    "href": "writeup.html#summary-of-findings",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "6.1 Summary of Findings",
    "text": "6.1 Summary of Findings\nThe project successfully accomplished its goals:\n\nTop Predictors: Price per KM and Mileage dominate price prediction.\nAccuracy: Random Forest achieved strong predictive performance (R² = 0.9952).\nStability: Low MAE on forward-looking data confirms the model is reliable for real-world forecasting."
  },
  {
    "objectID": "writeup.html#recommended-business-actions",
    "href": "writeup.html#recommended-business-actions",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "6.2 Recommended Business Actions",
    "text": "6.2 Recommended Business Actions\n\n1. Pricing Strategy\nIntegrate Price per KM into automated valuation tools. The low MAE ($531.13) supports high-confidence pricing automation.\n\n\n2. Inventory Optimization\nUse regional patterns from EDA to guide stocking and marketing:\n\nTarget best-performing models per region\nAvoid low-turnover variants\n\n\n\n3. Annual Temporal Validation\nRe-run the model yearly with updated splits:\n\nEnsures adaptation to new economic shifts\nMaintains predictive performance\nDetects structural breaks in market demand"
  },
  {
    "objectID": "writeup.html#future-work-and-response-to-peer-review",
    "href": "writeup.html#future-work-and-response-to-peer-review",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "7. Future Work and Response to Peer Review",
    "text": "7. Future Work and Response to Peer Review\nThe peer review provided insightful suggestions for enhancing the project’s methodological rigor and practical application. We plan to incorporate the following steps in future development:\n\nEnhanced Hyperparameter Tuning: We acknowledge the suggestion to split the training data (2010–2022) into separate internal training and validation sets for dedicated hyperparameter tuning. While the current model was robust, this practice will be adopted to systematically optimize the Random Forest Regressor and prevent any potential overfitting on the primary training set before the final temporal test.\nModel Deployment: As suggested, the logical next step is deployment. The trained model (rf_best_model.joblib) will be implemented within a simple web dashboard (like the simulated interface provided in the peer response) to allow non-technical users to interactively input car features and immediately receive a price prediction. This will demonstrate the model’s direct value as a real-time pricing tool."
  },
  {
    "objectID": "presentation.html#quarto",
    "href": "presentation.html#quarto",
    "title": "Project title",
    "section": "Quarto",
    "text": "Quarto\n\nThe presentation is created using the Quarto CLI\n## sets the start of a new slide"
  },
  {
    "objectID": "presentation.html#layouts",
    "href": "presentation.html#layouts",
    "title": "Project title",
    "section": "Layouts",
    "text": "Layouts\nYou can use plain text\n\n\n\nor bullet points1\n\n\nor in two columns\n\n\nlike\nthis\n\nAnd add footnotes"
  },
  {
    "objectID": "presentation.html#code",
    "href": "presentation.html#code",
    "title": "Project title",
    "section": "Code",
    "text": "Code\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.073\nModel:                            OLS   Adj. R-squared:                  0.070\nMethod:                 Least Squares   F-statistic:                     30.59\nDate:                Wed, 10 Dec 2025   Prob (F-statistic):           5.84e-08\nTime:                        15:09:11   Log-Likelihood:                -1346.4\nNo. Observations:                 392   AIC:                             2697.\nDf Residuals:                     390   BIC:                             2705.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         35.8015      2.266     15.800      0.000      31.347      40.257\nspeed       -354.7055     64.129     -5.531      0.000    -480.788    -228.623\n==============================================================================\nOmnibus:                       27.687   Durbin-Watson:                   0.589\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               18.976\nSkew:                           0.420   Prob(JB):                     7.57e-05\nKurtosis:                       2.323   Cond. No.                         169.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "presentation.html#plots",
    "href": "presentation.html#plots",
    "title": "Project title",
    "section": "Plots",
    "text": "Plots"
  },
  {
    "objectID": "presentation.html#plot-and-text",
    "href": "presentation.html#plot-and-text",
    "title": "Project title",
    "section": "Plot and text",
    "text": "Plot and text\n\n\n\nSome text\ngoes here"
  },
  {
    "objectID": "presentation.html#tables",
    "href": "presentation.html#tables",
    "title": "Project title",
    "section": "Tables",
    "text": "Tables\nIf you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\n\n\n\nisland\n\n\n\nbill_length_mm\n\n\n\nbill_depth_mm\n\n\n\nflipper_length_mm\n\n\n\nbody_mass_g\n\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.1\n\n\n\n18.7\n\n\n\n181.0\n\n\n\n3750.0\n\n\n\nMale\n\n\n\n\n\n\n\n1\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.5\n\n\n\n17.4\n\n\n\n186.0\n\n\n\n3800.0\n\n\n\nFemale\n\n\n\n\n\n\n\n2\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n40.3\n\n\n\n18.0\n\n\n\n195.0\n\n\n\n3250.0\n\n\n\nFemale\n\n\n\n\n\n\n\n4\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n36.7\n\n\n\n19.3\n\n\n\n193.0\n\n\n\n3450.0\n\n\n\nFemale\n\n\n\n\n\n\n\n5\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.3\n\n\n\n20.6\n\n\n\n190.0\n\n\n\n3650.0\n\n\n\nMale"
  },
  {
    "objectID": "presentation.html#images",
    "href": "presentation.html#images",
    "title": "Project title",
    "section": "Images",
    "text": "Images\n\nImage credit: Danielle Navarro, Percolate."
  },
  {
    "objectID": "presentation.html#math-expressions",
    "href": "presentation.html#math-expressions",
    "title": "Project title",
    "section": "Math Expressions",
    "text": "Math Expressions\nYou can write LaTeX math expressions inside a pair of dollar signs, e.g. $\\alpha+\\beta$ renders \\(\\alpha + \\beta\\). You can use the display style with double dollar signs:\n$$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$$\n\\[\n\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\]\nLimitations:\n\nThe source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting $$ must appear in the very beginning of a line, followed immediately by a non-space character, and the ending $$ must be at the end of a line, led by a non-space character;\nThere should not be spaces after the opening $ or before the closing $."
  },
  {
    "objectID": "presentation.html#feeling-adventurous",
    "href": "presentation.html#feeling-adventurous",
    "title": "Project title",
    "section": "Feeling adventurous?",
    "text": "Feeling adventurous?\n\nYou are welcomed to use the default styling of the slides. In fact, that’s what I expect majority of you will do. You will differentiate yourself with the content of your presentation.\nBut some of you might want to play around with slide styling. Some solutions for this can be found at https://quarto.org/docs/presentations/revealjs."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BMW Car Price Analysis & Prediction",
    "section": "",
    "text": "This project focuses on analyzing worldwide BMW car sales data from 2010 to 2024 to understand the complex factors that determine a vehicle’s market value. We conduct extensive Exploratory Data Analysis (EDA) to identify key price drivers, including mileage, engine size, car age, and regional sales trends. The core of the project involves building a robust machine learning model (Random Forest Regressor) to accurately predict car prices. Crucially, the model’s performance is validated using a temporal train-test split, simulating a real-world scenario where the model predicts 2024 prices based on data up to 2023. The final model achieves a strong temporal stability with a Mean Absolute Error (MAE) of approximately $531.13, providing high-confidence insights for pricing and inventory optimization strategies."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "BMW Car Price Analysis & Prediction",
    "section": "",
    "text": "This project focuses on analyzing worldwide BMW car sales data from 2010 to 2024 to understand the complex factors that determine a vehicle’s market value. We conduct extensive Exploratory Data Analysis (EDA) to identify key price drivers, including mileage, engine size, car age, and regional sales trends. The core of the project involves building a robust machine learning model (Random Forest Regressor) to accurately predict car prices. Crucially, the model’s performance is validated using a temporal train-test split, simulating a real-world scenario where the model predicts 2024 prices based on data up to 2023. The final model achieves a strong temporal stability with a Mean Absolute Error (MAE) of approximately $531.13, providing high-confidence insights for pricing and inventory optimization strategies."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "BMW Car Price Analysis & Prediction",
    "section": "",
    "text": "This project was developed by [Min Set Khant (Solo)] For INFO 523 - Data Mining and Discovery at the University of Arizona, taught by Dr. John Chen. The team is comprised of the following team members.\n\nTeam member 1: Min Set Khant (Solo Project) , Master Student of University of Arizona"
  },
  {
    "objectID": "Final_Presentation.html#introduction",
    "href": "Final_Presentation.html#introduction",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "Introduction",
    "text": "Introduction\nProject: BMW Car Sales — Data Analysis & Price Prediction\n\nGoal: Build a robust, temporally validated model to predict BMW car prices using multi-regional sales data.\nKey Questions Addressed:\n\nWhat are the key features influencing BMW car prices?\nCan a Machine Learning model accurately predict prices?\nHow stable are these predictions over time (Temporal Validation)?"
  },
  {
    "objectID": "Final_Presentation.html#data-overview-setup",
    "href": "Final_Presentation.html#data-overview-setup",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "Data Overview & Setup",
    "text": "Data Overview & Setup\nDataset: BMW Worldwide Sales (2010–2024)\n\nSource: Kaggle (Multi-Regional Sales Data)\nStatus: Data has been cleaned, imputed, and One-Hot Encoded.\nTarget Variable: Price_USD (Log-transformed for modeling).\n\n\n\n\n\n\n\n\n\n\nModel\nYear\nRegion\nColor\nFuel_Type\nTransmission\nEngine_Size_L\nMileage_KM\nPrice_USD\nSales_Volume\nSales_Classification\n\n\n\n\n0\n5 Series\n2016\nAsia\nRed\nPetrol\nManual\n3.5\n151748\n98740\n8300\nHigh\n\n\n1\ni8\n2013\nNorth America\nRed\nHybrid\nAutomatic\n1.6\n121671\n79219\n3428\nLow\n\n\n2\n5 Series\n2022\nNorth America\nBlue\nPetrol\nAutomatic\n4.5\n10991\n113265\n6994\nLow\n\n\n3\nX3\n2024\nMiddle East\nBlue\nPetrol\nAutomatic\n1.7\n27255\n60971\n4047\nLow\n\n\n4\n7 Series\n2020\nSouth America\nBlack\nDiesel\nManual\n2.1\n122131\n49898\n3080\nLow\n\n\n5\n5 Series\n2017\nMiddle East\nSilver\nDiesel\nManual\n1.9\n171362\n42926\n1232\nLow\n\n\n6\ni8\n2022\nEurope\nWhite\nDiesel\nManual\n1.8\n196741\n55064\n7949\nHigh\n\n\n7\nM5\n2014\nAsia\nBlack\nDiesel\nAutomatic\n1.6\n121156\n102778\n632\nLow\n\n\n8\nX3\n2016\nSouth America\nWhite\nDiesel\nAutomatic\n1.7\n48073\n116482\n8944\nHigh\n\n\n9\ni8\n2019\nEurope\nWhite\nElectric\nManual\n3.0\n35700\n96257\n4411\nLow"
  },
  {
    "objectID": "Final_Presentation.html#eda-market-trends-segmentation",
    "href": "Final_Presentation.html#eda-market-trends-segmentation",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "EDA: Market Trends & Segmentation",
    "text": "EDA: Market Trends & Segmentation\nRegional Performance and Price Trends\n\n\n\n\n\n\n\nSales Trend Over Time\nTop Models by Regional Sales Volume\n\n\n\n\n\n\n\n\nObservation: The average price shows a clear, non-linear upward trend over time, suggesting market inflation. This justifies the Temporal Validation strategy.\nObservation: Model popularity is highly segmented by region. This confirms that the Region variable is a critical feature for effective price segmentation.\n\n\n\nDetail: The analysis utilized over 50,000 records of multi-regional sales. Regional features were processed using One-Hot Encoding to prevent models from assigning arbitrary ordinal relationships to categories."
  },
  {
    "objectID": "Final_Presentation.html#global-market-demand",
    "href": "Final_Presentation.html#global-market-demand",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "Global Market Demand",
    "text": "Global Market Demand\nGlobal Market Demand: Top 10 Models (2010–2024)\n\n\n\n\n\n\nVolume by Model (2010–2024)\n\n\n\n\n\n\n\nInsight:Globally, sales volume is often dominated by core sedan and SUV series (e.g., 3-Series, 5-Series, X3, X5), reflecting high overall market liquidity.\n\n\n\nImpact:Models with high global volume typically exhibit more stable pricing due to consistent demand, which reduces prediction volatility. EDA Detail:This volume data was analyzed before the temporal split to understand the underlying market foundation across all years. |"
  },
  {
    "objectID": "Final_Presentation.html#eda-correlation-analysis",
    "href": "Final_Presentation.html#eda-correlation-analysis",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "EDA: Correlation Analysis",
    "text": "EDA: Correlation Analysis\nPrice Relationships\n\n\n\n\n\n\n\nCorrelation Matrix\nPrice Distribution by Fuel Type\n\n\n\n\n\n\n\n\nObservation: Log-Price shows a strong inverse linear relationship with Car Age (depreciation) and a strong positive correlation with Engine Size (performance/class).\nObservation: Significant differences in the median price across fuel types confirm this categorical feature has strong predictive power and is not independent of the target.\n\n\n\nDetail: Two key features were engineered for this analysis: Car Age (calculated as Current_Year - Model_Year) and Price per KM (calculated as Price_USD / Mileage_KM), both showing significant predictive value."
  },
  {
    "objectID": "Final_Presentation.html#modeling-approach",
    "href": "Final_Presentation.html#modeling-approach",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "Modeling Approach",
    "text": "Modeling Approach\nTemporal Validation Strategy (The Key Differentiator)\n\nGoal: Test model stability and real-world predictive ability.\nSplit: - Train Set: All data from 2010–2023.\n\nTest Set: Data from Year 2024 (The “future” market).\n\nModel Chosen: Random Forest Regressor (highest predictive power).\nEvaluation: MAE and RMSE are converted back to USD for business interpretability."
  },
  {
    "objectID": "Final_Presentation.html#random-forest-performance",
    "href": "Final_Presentation.html#random-forest-performance",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "Random Forest Performance",
    "text": "Random Forest Performance\nModel Accuracy (Reporting in USD)\n\n\n\n\n\n\n\n\nMetric\nValue\nInterpretation\n\n\n\n\nR-squared (Log Price)\n0.9952\nVariance explained by the model on the transformed price.\n\n\nRoot Mean Squared Error (RMSE)\n$2,292.13\nAverage prediction error, penalizing large errors.\n\n\nMean Absolute Error (MAE)\n$531.13\nMost Interpretable: On average, the model’s price prediction is off by this amount in USD.\n\n\n\nInterpretation: The model successfully predicted 2024 prices with an average absolute error of approximately $531.13, proving its temporal stability."
  },
  {
    "objectID": "Final_Presentation.html#key-price-drivers-feature-importance",
    "href": "Final_Presentation.html#key-price-drivers-feature-importance",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "Key Price Drivers: Feature Importance",
    "text": "Key Price Drivers: Feature Importance\nTop 5 Predictors of BMW Price\nThe Random Forest model identified these key drivers based on Gini Impurity Reduction:\n\nPrice per KM (0.682) - Represents the calculated efficiency/condition ratio; its dominance suggests it captures the residual value most effectively.\nMileage (KM) (0.318) - Primary measure of vehicle use and wear, directly driving the rate of depreciation.\nEngine Size (L) (0.000018)\nCar Age (0.000015)\nTransmission: Manual (0.000004)"
  },
  {
    "objectID": "Final_Presentation.html#conclusion-summary-of-findings",
    "href": "Final_Presentation.html#conclusion-summary-of-findings",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "Conclusion: Summary of Findings",
    "text": "Conclusion: Summary of Findings\nAnswering the Key Questions\n\nTop Drivers: Price is overwhelmingly driven by engineered features: Price per KM and Mileage (KM).\nModel Success: The Random Forest model demonstrates strong temporal validation, accurately predicting 2024 market prices with an MAE of $531.13.\nMarket Insight: Model popularity and sales volume are highly specific to Region."
  },
  {
    "objectID": "Final_Presentation.html#business-recommendations",
    "href": "Final_Presentation.html#business-recommendations",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "Business Recommendations",
    "text": "Business Recommendations\nActionable Insights for Strategy\n\nPricing Strategy: Use the model’s feature importance to set pricing guidelines. The low MAE ($531.13) provides high confidence for automated pricing models.\nInventory Optimization: Focus inventory based on the regional popularity derived from the EDA.\nFuture Validation: Re-run the temporal validation annually to ensure the model’s stability and adapt to market shifts."
  },
  {
    "objectID": "Final_Presentation.html#thank-you",
    "href": "Final_Presentation.html#thank-you",
    "title": "BMW Car Sales Analysis & Price Prediction",
    "section": "Thank You",
    "text": "Thank You\nINFO-523 Final Project\n\nNotebooks: 01_Data_Preparation_and_EDA.ipynb, 02-modeling.ipynb\nOutput: Trained model saved as rf_best_model.joblib.\nData: Cleaned data saved as bmw_cleaned.csv and bmw_modeling_ready.csv."
  },
  {
    "objectID": "all_notebook/02.modeling.html",
    "href": "all_notebook/02.modeling.html",
    "title": "BMW Car Sales — Modeling and Prediction",
    "section": "",
    "text": "INFO-523 Final Project | Min Set Khant (Solo)\n\nThis notebook loads the modeling-ready data, performs a temporal train-test split (up to 2023 for training, 2024 for testing), trains Linear Regression and Random Forest models, and evaluates performance on the original USD price scale.\n\n# 1. SETUP AND IMPORTS\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport os\nimport joblib \n\n# Scikit-learn modules for modeling and evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n# Define Paths\n# OUTPUT_DIR is needed to save the final model and prediction results\nOUTPUT_DIR = Path(\"..\") / \"data\" / \"cleaned\"\nVISUALS_DIR = Path(\"..\") / \"visuals\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(VISUALS_DIR, exist_ok=True)\n\n\n# 2. DATA LOADING\nDATA_PATH = Path(\"..\") / \"data\" / \"cleaned\" / \"bmw_modeling_ready.csv\"\n\ndf = pd.read_csv(DATA_PATH)\nprint(\"Modeling data shape:\", df.shape)\ndf.head()\n\nModeling data shape: (50000, 33)\n\n\n\n\n\n\n\n\n\nYear\nEngine_Size_L\nMileage_KM\nPrice_USD\nSales_Volume\nCar_Age\nlog_Price_USD\nPrice_per_KM\nTotal_Sales_Model\nRegion_Asia\n...\nModel_i8\nFuel_Type_Electric\nFuel_Type_Hybrid\nFuel_Type_Petrol\nTransmission_Manual\nColor_Blue\nColor_Grey\nColor_Red\nColor_Silver\nColor_White\n\n\n\n\n0\n2016\n3.5\n151748\n98740\n8300\n9\n11.500256\n0.650684\n23097519\nTrue\n...\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1\n2013\n1.6\n121671\n79219\n3428\n12\n11.279984\n0.651092\n23423891\nFalse\n...\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n2\n2022\n4.5\n10991\n113265\n6994\n3\n11.637494\n10.305250\n23097519\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n3\n2024\n1.7\n27255\n60971\n4047\n1\n11.018170\n2.237057\n22745529\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n4\n2020\n2.1\n122131\n49898\n3080\n5\n10.817756\n0.408561\n23786466\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 33 columns\n\n\n\n\n\n\n3. Temporal Train-Test Split (Predicting the Future)\nThe most critical part of the validation: we use a temporal split to validate the model’s ability to predict the newest market (Year 2024) based on all historical data (2010-2023).\n\n# Define Target (y) and Features (X)\nTARGET = \"log_Price_USD\"\n\n# X contains all features, including Year and Price_USD (needed for the split and final error)\nX = df.drop(columns=[TARGET])\ny = df[TARGET]\n\n# --- TEMPORAL SPLIT IMPLEMENTATION ---\nTEST_YEAR = 2024\n\n# Split data based on the Year column\nX_train = X[X[\"Year\"] &lt; TEST_YEAR]\nX_test = X[X[\"Year\"] == TEST_YEAR]\n\ny_train = y[X_train.index]\ny_test = y[X_test.index]\n\n# Store the original USD prices for the test set for final error calculation\ny_test_usd_true = X_test[\"Price_USD\"]\n\n# Clean up X_train/X_test by dropping redundant/leaking columns before training\n# Price_USD is a direct leak, Year is only for the split, Sales_Volume is typically post-sale info\ncols_to_drop = [\"Year\", \"Price_USD\", \"Sales_Volume\", \"Total_Sales_Model\"] \nX_train = X_train.drop(columns=[c for c in cols_to_drop if c in X_train.columns], errors='ignore')\nX_test = X_test.drop(columns=[c for c in cols_to_drop if c in X_test.columns], errors='ignore')\n\nprint(f\"Training Data (Years &lt; {TEST_YEAR}): {X_train.shape[0]} rows\")\nprint(f\"Testing Data (Year == {TEST_YEAR}): {X_test.shape[0]} rows\")\n\nTraining Data (Years &lt; 2024): 46573 rows\nTesting Data (Year == 2024): 3427 rows\n\n\n\n\n4. Model Training and Evaluation\nThe evaluation function converts predictions back to USD using np.expm1() so that MAE and RMSE are directly usable by a business audience.\n\n# Helper function to evaluate models and print results\ndef evaluate_model(model_name, y_true_log, y_pred_log, y_test_usd_true):\n    \n    # 1. Convert log-predictions back to USD (Original Price Scale)\n    y_pred_usd = np.expm1(y_pred_log)\n    \n    # 2. Calculate metrics\n    r2 = r2_score(y_true_log, y_pred_log) \n    rmse = np.sqrt(mean_squared_error(y_test_usd_true, y_pred_usd))\n    mae = mean_absolute_error(y_test_usd_true, y_pred_usd)\n    \n    print(f\"{model_name} Results (Temporal Split, Test Year {TEST_YEAR}):\")\n    print(f\"  R-squared (Log Price): {r2:.4f} - Variance explained\")\n    print(f\"  RMSE (USD): ${rmse:,.2f} - Average Price Error (Higher Penalty to Large Errors)\")\n    print(f\"  MAE (USD): ${mae:,.2f} - Median Price Error (Most interpretable)\")\n    print(\"\")\n    \n    return r2, rmse, mae\n\n\n# Model 1: Linear Regression\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\nlr_pred = lr_model.predict(X_test)\n\nlr_r2, lr_rmse, lr_mae = evaluate_model(\n    \"Linear Regression\", \n    y_test, \n    lr_pred, \n    y_test_usd_true\n)\n\nLinear Regression Results (Temporal Split, Test Year 2024):\n  R-squared (Log Price): 0.0003 - Variance explained\n  RMSE (USD): $26,490.86 - Average Price Error (Higher Penalty to Large Errors)\n  MAE (USD): $22,927.30 - Median Price Error (Most interpretable)\n\n\n\n\n# Model 2: Random Forest Regressor \n\nrf_model = RandomForestRegressor(\n    n_estimators=100, \n    random_state=42, \n    n_jobs=-1, \n    max_depth=15, \n    min_samples_leaf=5\n)\nrf_model.fit(X_train, y_train)\nrf_pred = rf_model.predict(X_test)\n\nrf_r2, rf_rmse, rf_mae = evaluate_model(\n    \"Random Forest Regressor\", \n    y_test, \n    rf_pred, \n    y_test_usd_true\n)\n\nRandom Forest Regressor Results (Temporal Split, Test Year 2024):\n  R-squared (Log Price): 0.9952 - Variance explained\n  RMSE (USD): $2,292.13 - Average Price Error (Higher Penalty to Large Errors)\n  MAE (USD): $531.13 - Median Price Error (Most interpretable)\n\n\n\n\n\n5. Feature Importance and Model Insights\nAnalyzing the feature importance from the Random Forest model reveals the key drivers of BMW car price, directly addressing the first proposal question.\n\n# Random Forest Feature Importance\nimportances = pd.Series(rf_model.feature_importances_, index=X_train.columns)\ntop_20_features = importances.sort_values(ascending=False).head(20)\n\n# Visualization\nplt.figure(figsize=(12, 8))\nsns.barplot(x=top_20_features.values, y=top_20_features.index, palette=\"viridis\")\nplt.title(\"Top 20 Features Driving BMW Price Prediction (Random Forest)\", fontsize=16)\nplt.xlabel(\"Feature Importance Score (Gini)\")\nplt.ylabel(\"Feature\")\nplt.tight_layout()\nplt.savefig(VISUALS_DIR / \"rf_feature_importance.png\")\nplt.show()\n\nprint(\"\\nTop 5 Price Drivers (Feature Importance):\")\nprint(top_20_features.head(5))\n\n/var/folders/jf/1b91kr5d1k773y5y7tm0sfbh0000gn/T/ipykernel_32198/956784539.py:7: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=top_20_features.values, y=top_20_features.index, palette=\"viridis\")\n\n\n\n\n\n\n\n\n\n\nTop 5 Price Drivers (Feature Importance):\nPrice_per_KM           0.682248\nMileage_KM             0.317700\nEngine_Size_L          0.000018\nCar_Age                0.000015\nTransmission_Manual    0.000004\ndtype: float64\n\n\n\n\n6. Final Model Deployment Function\nThis conceptual function demonstrates how the final model can be wrapped for real-world use, ensuring the input matches the training features and the output is the predicted price in USD.\n\n# Final Model Deployment Function\ndef predict_bmw_price(car_features_dict, rf_model_temp):\n    \"\"\"\n    Predicts the price of a BMW given its features.\n    \n    NOTE: The input dictionary must contain all feature columns \n    (including OHE columns) that the model was trained on (X_train).\n    \"\"\"\n    # Convert dictionary to DataFrame for model input\n    df_new = pd.DataFrame([car_features_dict])\n    \n    # Ensure all columns exist and are in the correct order (crucial for OHE data)\n    required_cols = X_train.columns\n    for col in required_cols:\n        if col not in df_new.columns:\n            df_new[col] = 0\n            \n    df_new = df_new[required_cols]\n    \n    # Predict on the log scale\n    price_pred_log = rf_model_temp.predict(df_new)[0]\n    \n    # Convert back to USD using the inverse of log1p\n    price_pred_usd = np.expm1(price_pred_log)\n    \n    return price_pred_usd\n\n# Example usage (Conceptual: you would replace this with actual input data)\n# example_car_dict = {\n#     'Engine_Size_L': 3.0, 'Mileage_KM': 10000, 'Car_Age': 1, 'Price_per_KM': 8.5, \n#     'Region_Asia': 1, 'Region_Europe': 0, ... (all 100+ features set to 0 or 1)\n# }\n# print(f\"Predicted Price (USD): ${predict_bmw_price(example_car_dict, rf_model):,.2f}\")\n\n\n# 7. Saving Model and Prediction Proofs\n\n# 1. Saving the Trained Random Forest Model\nMODEL_SAVE_PATH = OUTPUT_DIR / \"rf_best_model.joblib\" \n\n# Save the model object\njoblib.dump(rf_model, MODEL_SAVE_PATH)\nprint(f\"✅ Saved the trained Random Forest model to: {MODEL_SAVE_PATH}\")\n\n\n# 2. Saving the Temporal Prediction Results\n# y_pred_usd was calculated in the evaluate_model function\ny_pred_usd = np.expm1(rf_pred)\n\n# Retrieve the actual USD prices and create a comparison DataFrame\nresults_df = pd.DataFrame({\n    'Actual_Price_USD': y_test_usd_true,\n    'Predicted_Price_USD': y_pred_usd,\n    'Prediction_Error_USD': np.abs(y_test_usd_true - y_pred_usd)\n})\n\nresults_df['Year'] = TEST_YEAR\nresults_df = results_df.sort_values(by='Prediction_Error_USD', ascending=False)\n\n# Define the path and save the results\nRESULTS_SAVE_PATH = OUTPUT_DIR / \"temporal_predictions_2024.csv\"\nresults_df.to_csv(RESULTS_SAVE_PATH, index=False)\nprint(f\"✅ Saved temporal prediction results to: {RESULTS_SAVE_PATH}\")\n\nprint(\"\\nTop 5 Largest Prediction Errors (for inspection):\")\ndisplay(results_df.head())\n\n✅ Saved the trained Random Forest model to: ../data/cleaned/rf_best_model.joblib\n✅ Saved temporal prediction results to: ../data/cleaned/temporal_predictions_2024.csv\n\nTop 5 Largest Prediction Errors (for inspection):\n\n\n\n\n\n\n\n\n\nActual_Price_USD\nPredicted_Price_USD\nPrediction_Error_USD\nYear\n\n\n\n\n762\n114283\n76473.818456\n37809.181544\n2024\n\n\n483\n119117\n83383.235095\n35733.764905\n2024\n\n\n40586\n116759\n83383.235095\n33375.764905\n2024\n\n\n2101\n109818\n77051.262024\n32766.737976\n2024\n\n\n37022\n108646\n76714.311586\n31931.688414\n2024"
  }
]